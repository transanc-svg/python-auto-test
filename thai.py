import feedparser
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import os
import requests

# --- Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆèªè¨¼ ---
scope = ["https://spreadsheets.google.com/feeds",
         "https://www.googleapis.com/auth/drive"]

print("ğŸ”‘ Googleèªè¨¼é–‹å§‹...")
google_creds = os.environ["GOOGLE_CREDENTIALS"]
creds_dict = json.loads(google_creds)
creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
client = gspread.authorize(creds)
print("âœ… Googleèªè¨¼æˆåŠŸ")

spreadsheet_name = "ã‚¿ã‚¤"
sheet = client.open(spreadsheet_name).sheet1

# --- æ—¢å­˜ã®URLã‚’å–å¾—ã—ã¦é‡è¤‡é˜²æ­¢ ---
existing_urls = sheet.col_values(2)  # Båˆ—
existing_urls = existing_urls[1:] if existing_urls else []

# --- ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ  ---
if not existing_urls:
    sheet.append_row(["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°", "èª¬æ˜", "ç”»åƒURL"])

# --- RSSãƒ•ã‚£ãƒ¼ãƒ‰ ---
rss_url = "https://news.google.com/rss/topics/CAAqIQgKIhtDQkFTRGdvSUwyMHZNRGRtTVhnU0FtcGhLQUFQAQ?hl=ja&gl=JP&ceid=JP%3Aja&oc=11"
feed = feedparser.parse(rss_url)
entries_to_process = feed.entries[:30][::-1]  # å¤ã„é †ã«æœ€å¤§30ä»¶

# --- Selenium ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
options = Options()
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
print("âœ… Chromeèµ·å‹•æˆåŠŸ")

VALID_EXTENSIONS = (".jpg", ".jpeg", ".png", ".webp")
EXCLUDE_DOMAINS = [
    "jp.fashionnetwork.com", "newscast.jp",
    "www.keidanren.or.jp", "ashu-aseanstatistics.com"
]

# --- TextRazor APIã‚­ãƒ¼ ---
TEXTRAZOR_API_KEY = "fbedccf39739132e30c41096f166561c9cfb85bc36b44c1c16c8b8a2"

def generate_hashtags(text):
    """TextRazor APIã§ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç”Ÿæˆ"""
    url = "https://api.textrazor.com/"
    payload = {"text": text, "extractors": "entities,topics,words"}
    headers = {"X-TextRazor-Key": TEXTRAZOR_API_KEY, "Content-Type": "application/x-www-form-urlencoded"}
    data = "&".join([f"{k}={requests.utils.quote(v)}" for k, v in payload.items()])
    try:
        res = requests.post(url, headers=headers, data=data, timeout=10)
        if res.status_code != 200:
            return "#ã‚¿ã‚¤ #ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        data_json = res.json()
        entities = data_json.get("response", {}).get("entities", [])
        tags = []
        for e in sorted(entities, key=lambda x: x.get("confidenceScore", 0), reverse=True):
            tag = e.get("entityId") or e.get("matchedText")
            if tag:
                tag = tag.replace(" ", "")
                if tag not in tags:
                    tags.append(tag)
        if len(tags) < 5:
            for w in data_json.get("response", {}).get("words", []):
                tag = w.get("token")
                if tag and tag not in tags:
                    tags.append(tag)
                if len(tags) >= 5:
                    break
        tags = [t for t in tags if not any(c.isdigit() for c in t)]
        return " ".join(f"#{t}" for t in tags[:5]) if tags else "#ã‚¿ã‚¤ #ãƒ‹ãƒ¥ãƒ¼ã‚¹"
    except Exception as e:
        print(f"TextRazor API error: {e}")
        return "#ã‚¿ã‚¤ #ãƒ‹ãƒ¥ãƒ¼ã‚¹"

# --- RSSå‡¦ç† ---
for entry in entries_to_process:
    title = entry.title
    google_url = entry.link

    if any(domain in google_url for domain in EXCLUDE_DOMAINS):
        print(f"é™¤å¤–ã‚¹ã‚­ãƒƒãƒ—: {title} â†’ {google_url}")
        continue

    print(f"â–¶ {title}")
    print(f"  URL: {google_url}")

    try:
        driver.get(google_url)
        # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")
        time.sleep(3)
        original_url = driver.current_url

        if any(domain in original_url for domain in EXCLUDE_DOMAINS):
            print(f"â­ é™¤å¤–ã‚¹ã‚­ãƒƒãƒ—: {original_url}")
            continue

        # og:image å–å¾—
        image_url = None
        try:
            og_image_element = driver.find_element(By.XPATH, "//meta[@property='og:image']")
            image_url = og_image_element.get_attribute("content")
            if (not image_url or not image_url.startswith("http")):
                image_url = None
        except:
            image_url = None

        # description å–å¾—
        description = ""
        try:
            desc_element = driver.find_element(By.XPATH, "//meta[@name='description']")
            description = desc_element.get_attribute("content") or ""
        except:
            description = ""

        print(f"  ç”»åƒURL: {image_url}")
        print(f"  èª¬æ˜: {description[:60]}...")
        print(f"  ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¾ŒURL: {original_url}")

    except Exception as e:
        print(f"âš ï¸ URLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {title} â†’ {e}")
        original_url = google_url
        image_url = None
        description = ""

    # --- æ›¸ãè¾¼ã¿åˆ¤å®š ---
    if image_url and original_url not in existing_urls:
        hashtags = generate_hashtags(title)
        sheet.append_row([title, original_url, hashtags, description, image_url])
        existing_urls.append(original_url)
        print(f"âœ… è¿½åŠ : {title}")
    else:
        print(f"â­ ã‚¹ã‚­ãƒƒãƒ—: {title}ï¼ˆç”»åƒãªã— or æ—¢å­˜URLï¼‰")

driver.quit()
print("ğŸ‰ å®Œäº†: æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
